#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ”¹è¿›çš„X-ray Groundingä»»åŠ¡è¯„ä¼°è„šæœ¬
æ­£ç¡®å¤„ç†ç›¸å¯¹åæ ‡æ ¼å¼å’Œä¸¤ç§æƒ…å†µçš„è¯„åˆ¤æ ‡å‡†
æ”¯æŒä»<answer></answer>æ ‡ç­¾ä¸­æå–ç­”æ¡ˆ
"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "3"
import sys
import json
import re
import argparse
import numpy as np
from typing import List, Dict, Tuple, Any, Optional
from pathlib import Path
from tqdm import tqdm
import torch
from PIL import Image
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ LLaMA-Factoryè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def extract_answer_from_tags(text: str) -> str:
    """
    ä»æ–‡æœ¬ä¸­æå–<answer></answer>æ ‡ç­¾å†…çš„å†…å®¹
    å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
    """
    answer_pattern = r'<answer>(.*?)</answer>'
    match = re.search(answer_pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()

def parse_bbox_from_text(text: str) -> List[Tuple[float, float, float, float]]:
    """
    ä»æ¨¡å‹è¾“å‡ºæ–‡æœ¬ä¸­è§£æbboxåæ ‡
    æ ¼å¼: <box>(x1,y1),(x2,y2)</box>
    æ”¯æŒç›¸å¯¹åæ ‡(0-1)å’Œç»å¯¹åæ ‡(>1)
    è¿”å›: [(x1, y1, x2, y2), ...] ç›¸å¯¹åæ ‡æ ¼å¼
    """
    # é¦–å…ˆæå–answeræ ‡ç­¾å†…çš„å†…å®¹
    answer_text = extract_answer_from_tags(text)
    print("answer_text:",answer_text)
    # ç»Ÿä¸€çš„åæ ‡åŒ¹é…æ ¼å¼ï¼Œæ”¯æŒæ•´æ•°å’Œå°æ•°
    box_pattern = r'<box>\(([0-9.]+),([0-9.]+)\),\(([0-9.]+),([0-9.]+)\)</box>'
    matches = re.findall(box_pattern, answer_text)
    
    bboxes = []
    
    for match in matches:
        x1, y1, x2, y2 = map(float, match)
        
        # åˆ¤æ–­æ˜¯ç›¸å¯¹åæ ‡è¿˜æ˜¯ç»å¯¹åæ ‡
        # å¦‚æœæ‰€æœ‰åæ ‡éƒ½åœ¨0-1èŒƒå›´å†…ï¼Œè®¤ä¸ºæ˜¯ç›¸å¯¹åæ ‡
        if 0 <= x1 <= 1 and 0 <= y1 <= 1 and 0 <= x2 <= 1 and 0 <= y2 <= 1:
            # ç›¸å¯¹åæ ‡ï¼Œç›´æ¥ä½¿ç”¨
            x1_rel, y1_rel, x2_rel, y2_rel = x1, y1, x2, y2
        else:
            # ç»å¯¹åæ ‡ï¼Œè½¬æ¢ä¸ºç›¸å¯¹åæ ‡ï¼ˆé™¤ä»¥1000ï¼‰
            x1_rel = x1 / 1000.0
            y1_rel = y1 / 1000.0
            x2_rel = x2 / 1000.0
            y2_rel = y2 / 1000.0
            
            # æ£€æŸ¥è½¬æ¢åçš„åæ ‡æ˜¯å¦åˆç†
            if not (0 <= x1_rel <= 1 and 0 <= y1_rel <= 1 and 0 <= x2_rel <= 1 and 0 <= y2_rel <= 1):
                # å¦‚æœè½¬æ¢åä»ä¸åœ¨0-1èŒƒå›´å†…ï¼Œè·³è¿‡è¿™ä¸ªè¾¹ç•Œæ¡†
                continue
        
        # ç¡®ä¿åæ ‡é¡ºåºæ­£ç¡® (å·¦ä¸Šè§’åˆ°å³ä¸‹è§’)
        x1_rel, x2_rel = min(x1_rel, x2_rel), max(x1_rel, x2_rel)
        y1_rel, y2_rel = min(y1_rel, y2_rel), max(y1_rel, y2_rel)
        
        # æ£€æŸ¥è¾¹ç•Œæ¡†æ˜¯å¦æœ‰æ•ˆï¼ˆå®½åº¦å’Œé«˜åº¦å¤§äº0ï¼‰
        if x2_rel > x1_rel and y2_rel > y1_rel:
            bboxes.append((x1_rel, y1_rel, x2_rel, y2_rel))
    
    return bboxes

def calculate_iou(bbox1: Tuple[float, float, float, float], 
                  bbox2: Tuple[float, float, float, float]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªbboxçš„IoU
    bboxæ ¼å¼: (x1, y1, x2, y2) ç›¸å¯¹åæ ‡
    """
    x1_1, y1_1, x2_1, y2_1 = bbox1
    x1_2, y1_2, x2_2, y2_2 = bbox2
    
    # è®¡ç®—äº¤é›†
    x_left = max(x1_1, x1_2)
    y_top = max(y1_1, y1_2)
    x_right = min(x2_1, x2_2)
    y_bottom = min(y2_1, y2_2)
    
    if x_right < x_left or y_bottom < y_top:
        return 0.0
    
    intersection = (x_right - x_left) * (y_bottom - y_top)
    
    # è®¡ç®—å„è‡ªé¢ç§¯
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    
    # è®¡ç®—å¹¶é›†
    union = area1 + area2 - intersection
    
    if union == 0:
        return 0.0
    
    return intersection / union

def evaluate_detection_improved(pred_bboxes: List[Tuple[float, float, float, float]], 
                               gt_bboxes: List[Tuple[float, float, float, float]], 
                               pred_has_abnormality: bool,
                               gt_has_abnormality: bool,
                               iou_threshold: float = 0.5) -> Dict[str, Any]:
    """
    æ”¹è¿›çš„æ£€æµ‹è¯„ä¼°å‡½æ•°
    å¤„ç†ä¸¤ç§æƒ…å†µï¼šæœ‰æ¡†å’Œæ— æ¡†
    """
    # æƒ…å†µ1: çœŸå®æ— å¼‚å¸¸ï¼Œé¢„æµ‹æ— å¼‚å¸¸ -> æ­£ç¡®
    if not gt_has_abnormality and not pred_has_abnormality:
        return {
            "correct": True,
            "case": "true_negative",
            "precision": 1.0,
            "recall": 1.0,
            "f1": 1.0,
            "iou_score": 1.0
        }
    
    # æƒ…å†µ2: çœŸå®æ— å¼‚å¸¸ï¼Œé¢„æµ‹æœ‰å¼‚å¸¸ -> é”™è¯¯
    if not gt_has_abnormality and pred_has_abnormality:
        return {
            "correct": False,
            "case": "false_positive",
            "precision": 0.0,
            "recall": 1.0,
            "f1": 0.0,
            "iou_score": 0.0
        }
    
    # æƒ…å†µ3: çœŸå®æœ‰å¼‚å¸¸ï¼Œé¢„æµ‹æ— å¼‚å¸¸ -> é”™è¯¯
    if gt_has_abnormality and not pred_has_abnormality:
        return {
            "correct": False,
            "case": "false_negative",
            "precision": 1.0,
            "recall": 0.0,
            "f1": 0.0,
            "iou_score": 0.0
        }
    
    # æƒ…å†µ4: çœŸå®æœ‰å¼‚å¸¸ï¼Œé¢„æµ‹æœ‰å¼‚å¸¸ -> éœ€è¦è®¡ç®—IoU
    if gt_has_abnormality and pred_has_abnormality:
        if len(pred_bboxes) == 0 or len(gt_bboxes) == 0:
            return {
                "correct": False,
                "case": "bbox_mismatch",
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "iou_score": 0.0
            }
        
        # è®¡ç®—æ¯ä¸ªé¢„æµ‹æ¡†ä¸çœŸå®æ¡†çš„æœ€å¤§IoU
        matched_gt = set()
        true_positives = 0
        max_iou = 0.0
        
        for pred_bbox in pred_bboxes:
            pred_max_iou = 0.0
            best_gt_idx = -1
            
            for gt_idx, gt_bbox in enumerate(gt_bboxes):
                if gt_idx in matched_gt:
                    continue
                
                iou = calculate_iou(pred_bbox, gt_bbox)
                if iou > pred_max_iou:
                    pred_max_iou = iou
                    best_gt_idx = gt_idx
            
            if pred_max_iou > max_iou:
                max_iou = pred_max_iou
            
            if pred_max_iou >= iou_threshold and best_gt_idx != -1:
                matched_gt.add(best_gt_idx)
                true_positives += 1
        
        precision = true_positives / len(pred_bboxes) if len(pred_bboxes) > 0 else 0.0
        recall = true_positives / len(gt_bboxes) if len(gt_bboxes) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # åˆ¤æ–­æ˜¯å¦æ­£ç¡® (IoUé˜ˆå€¼å’ŒF1åˆ†æ•°)
        is_correct = max_iou >= iou_threshold and f1 > 0.5
        
        return {
            "correct": is_correct,
            "case": "true_positive" if is_correct else "low_iou_or_f1",
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "iou_score": max_iou,
            "max_iou": max_iou,
            "matched_pairs": true_positives,
            "total_predictions": len(pred_bboxes),
            "total_ground_truths": len(gt_bboxes)
        }
    
    # å…œåº•è¿”å›ï¼Œä¿®å¤linteré”™è¯¯
    return {
        "correct": False,
        "case": "unknown",
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "iou_score": 0.0
    }

def load_test_data_improved(test_file: str) -> List[Dict[str, Any]]:
    """
    æ”¹è¿›çš„æµ‹è¯•æ•°æ®åŠ è½½å‡½æ•°
    æ”¯æŒä»<answer></answer>æ ‡ç­¾ä¸­æå–ç­”æ¡ˆ
    """
    print(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    with open(test_file, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
    processed_data = []
    for item in raw_data:
        # ä»assistantçš„å›ç­”ä¸­æå–çœŸå®æ ‡æ³¨
        assistant_content = item["messages"][1]["content"]
        image_path = item["images"][0]
        
        # æå–answeræ ‡ç­¾å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        extracted_answer = extract_answer_from_tags(assistant_content)
        
        # è§£æçœŸå®æ ‡æ³¨
        gt_bboxes = parse_bbox_from_text(assistant_content)
        gt_has_abnormality = len(gt_bboxes) > 0
        
        # åˆ›å»ºç»Ÿä¸€æ ¼å¼çš„æ•°æ®é¡¹
        data_item = {
            "image": image_path,
            "annotations": [{"bbox": bbox} for bbox in gt_bboxes],  # ç›´æ¥ä½¿ç”¨ç›¸å¯¹åæ ‡
            "gt_bboxes_relative": gt_bboxes,  # ä¿å­˜ç›¸å¯¹åæ ‡
            "has_abnormality": gt_has_abnormality,
            "raw_annotation": assistant_content,
            "extracted_answer": extracted_answer  # ä¿å­˜æå–çš„ç­”æ¡ˆ
        }
        processed_data.append(data_item)
    
    print(f"æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(processed_data)}")
    print(f"  æœ‰ç—…å˜æ ·æœ¬: {sum(1 for item in processed_data if item['has_abnormality'])}")
    print(f"  æ— ç—…å˜æ ·æœ¬: {sum(1 for item in processed_data if not item['has_abnormality'])}")
    
    return processed_data

def parse_prediction_text_improved(text: str) -> Tuple[List[Tuple[float, float, float, float]], bool]:
    """
    æ”¹è¿›çš„é¢„æµ‹æ–‡æœ¬è§£æå‡½æ•°
    æ”¯æŒä»<answer></answer>æ ‡ç­¾ä¸­æå–ç­”æ¡ˆ
    """
    # é¦–å…ˆæå–answeræ ‡ç­¾å†…çš„å†…å®¹
    answer_text = extract_answer_from_tags(text)
    
    # æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¡¨ç¤ºæ²¡æœ‰æ£€æµ‹åˆ°å¼‚å¸¸
    no_finding_patterns = [
        "no lung opacity regions detected",
        "no abnormality detected",
        "no abnormal areas found",
        "no opacity regions found",
        "normal chest x-ray",
        "no findings",
        "no lesions detected"
    ]
    
    text_lower = answer_text.lower()
    has_no_finding = any(pattern in text_lower for pattern in no_finding_patterns)
    
    # è§£æbounding box
    bboxes = parse_bbox_from_text(text)
    
    # åˆ¤æ–­æ˜¯å¦æœ‰å¼‚å¸¸
    has_abnormality = len(bboxes) > 0 or not has_no_finding
    
    return bboxes, has_abnormality

def load_model_and_tokenizer(model_path: str):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    try:
        from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
        from qwen_vl_utils import process_vision_info
        
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
            device_map="auto",
            trust_remote_code=True
        )
        processor = Qwen2VLProcessor.from_pretrained(model_path, trust_remote_code=True)
        return model, processor
    except ImportError as e:
        print(f"å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…transformerså’Œqwen_vl_utilsåº“")
        return None, None

def generate_grounding_prediction(model, processor, image_path: str) -> str:
    """ç”Ÿæˆgroundingé¢„æµ‹ç»“æœ"""
    if model is None or processor is None:
        return ""
        
    try:
        # æ„å»ºgroundingä»»åŠ¡çš„prompt
        prompt = "<image>Please detect and locate any lung opacity regions in this chest X-ray image.\n\nDetection Guidelines:\n- Look for areas of increased density in the lung fields\n- Consider consolidation, infiltrates, or other opacity patterns\n- Ensure coordinates are within image boundaries\n- Provide precise (x1,y1),(x2,y2) coordinates\n\nOutput Format:\n\"The detected lung opacity regions are: Lung Opacity: <box>(x1,y1),(x2,y2)</box>\"\n\nIf no opacity is detected, output: \"The detected lung opacity regions are: No lung opacity regions detected.\""
        
        image = Image.open(image_path).convert('RGB')
        
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        try:
            from qwen_vl_utils import process_vision_info
            vision_output = process_vision_info(messages)
            if isinstance(vision_output, tuple) and len(vision_output) >= 2:
                image_inputs, video_inputs = vision_output[0], vision_output[1]
            else:
                image_inputs, video_inputs = vision_output, None
        except ImportError:
            print("è­¦å‘Š: æ— æ³•å¯¼å…¥qwen_vl_utilsï¼Œè·³è¿‡è§†è§‰ä¿¡æ¯å¤„ç†")
            return ""
            
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        inputs = inputs.to(model.device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=4096,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        response = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]
        
        return response.strip()
        
    except Exception as e:
        print(f"ç”Ÿæˆé¢„æµ‹æ—¶å‡ºé”™: {e}")
        return ""

def run_inference_improved(model_path: str, test_data: List[Dict[str, Any]], 
                          output_file: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    æ”¹è¿›çš„æ¨ç†å‡½æ•°
    """
    print("å¼€å§‹åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†...")
    
    # åŠ è½½æ¨¡å‹
    model, processor = load_model_and_tokenizer(model_path)
    
    if model is None or processor is None:
        print("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æ¨ç†")
        return []
    
    # è¿è¡Œæ¨ç†
    results = []
    for i, item in enumerate(tqdm(test_data, desc="æ¨ç†è¿›åº¦")):
        image_path = item["image"]
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            continue
        
        # ç”Ÿæˆé¢„æµ‹
        prediction = generate_grounding_prediction(model, processor, image_path)
        print("prediction:",prediction)
        
        # ä»é¢„æµ‹ç»“æœä¸­æå–answer
        extracted_prediction_answer = extract_answer_from_tags(prediction)
        
        result = {
            "image": item["image"],
            "ground_truth": item["annotations"],
            "gt_bboxes_relative": item["gt_bboxes_relative"],
            "gt_has_abnormality": item["has_abnormality"],
            "prediction": prediction,
            "raw_annotation": item.get("raw_annotation", ""),
            "extracted_answer": item.get("extracted_answer", ""),  # ä¿ç•™GTçš„extracted_answer
            "extracted_prediction_answer": extracted_prediction_answer  # æ–°å¢ï¼šä»é¢„æµ‹ä¸­æå–çš„answer
        }
        results.append(result)
        
        # æ‰“å°è¿›åº¦ä¿¡æ¯
        if (i + 1) % 10 == 0:
            print(f"å·²å¤„ç† {i + 1}/{len(test_data)} ä¸ªæ ·æœ¬")
    
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
    
    return results

def evaluate_results_improved(results: List[Dict[str, Any]], 
                             iou_thresholds: List[float] = [0.3, 0.5, 0.7]) -> Dict[str, Any]:
    """
    æ”¹è¿›çš„è¯„ä¼°å‡½æ•°
    """
    # æŒ‰IoUé˜ˆå€¼ç»Ÿè®¡çš„æŒ‡æ ‡
    iou_metrics = {}
    for iou_th in iou_thresholds:
        iou_metrics[f"iou_{iou_th}"] = {
            "correct_samples": 0,
            "total_samples": 0,
            "true_negatives": 0,
            "false_positives": 0,
            "false_negatives": 0,
            "true_positives": 0,
            "low_iou_or_f1": 0
        }
    
    # ä¸“é—¨ç»Ÿè®¡"çœŸå®æœ‰æ¡†ä¸”é¢„æµ‹æœ‰æ¡†"æƒ…å†µçš„IoUå€¼
    both_has_bbox_ious = []
    
    # é«˜IoUæ ·æœ¬æ”¶é›†ï¼ˆç”¨äºè®ºæ–‡å±•ç¤ºï¼‰
    high_iou_samples = []
    perfect_samples = []  # IoU > 0.9çš„æ ·æœ¬
    good_samples = []     # IoU > 0.7çš„æ ·æœ¬
    medium_samples = []   # IoU > 0.5çš„æ ·æœ¬
    
    # è¯¦ç»†ç»“æœ
    detailed_results = []
    
    # ç”¨äºå®æ—¶ç»Ÿè®¡mean_iouçš„å˜é‡
    all_ious = []
    samples_with_bbox = 0
    
    for i, result in enumerate(results):
        # è§£æé¢„æµ‹ç»“æœ
        pred_text = result["prediction"]
        pred_bboxes_relative, pred_has_abnormality = parse_prediction_text_improved(pred_text)
        
        # è·å–çœŸå®æ ‡æ³¨
        gt_bboxes_relative = result["gt_bboxes_relative"]
        gt_bboxes = [bbox for bbox in gt_bboxes_relative] # ç›´æ¥ä½¿ç”¨ç›¸å¯¹åæ ‡
        gt_has_abnormality = result["gt_has_abnormality"]
        
        # ä¸“é—¨è®¡ç®—"çœŸå®æœ‰æ¡†ä¸”é¢„æµ‹æœ‰æ¡†"æƒ…å†µçš„IoU
        if gt_has_abnormality and pred_has_abnormality and len(gt_bboxes) > 0 and len(pred_bboxes_relative) > 0:
            sample_ious = []
            for pred_bbox in pred_bboxes_relative:
                for gt_bbox in gt_bboxes:
                    iou = calculate_iou(pred_bbox, gt_bbox)
                    sample_ious.append(iou)
                    both_has_bbox_ious.append(iou)
                    all_ious.append(iou)  # æ·»åŠ åˆ°å…¨å±€IoUåˆ—è¡¨
            
            # è®¡ç®—è¯¥æ ·æœ¬çš„æœ€å¤§IoU
            if sample_ious:
                max_sample_iou = max(sample_ious)
            else:
                max_sample_iou = 0.0
            
            samples_with_bbox += 1
        else:
            max_sample_iou = 0.0
        
        # å®æ—¶æ‰“å°mean_iouï¼ˆæ¯10ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡ï¼‰
        if (i + 1) % 10 == 0 and all_ious:
            current_mean_iou = np.mean(all_ious)
            print(f"ğŸ“Š å·²å¤„ç† {i + 1}/{len(results)} ä¸ªæ ·æœ¬ï¼Œå½“å‰mean_iou: {current_mean_iou:.4f} (åŸºäº {len(all_ious)} ä¸ªIoUå€¼)")
        
        # æ”¶é›†é«˜IoUæ ·æœ¬ç”¨äºè®ºæ–‡å±•ç¤º
        if gt_has_abnormality and pred_has_abnormality and len(gt_bboxes) > 0 and len(pred_bboxes_relative) > 0:
            if max_sample_iou > 0.9:
                perfect_samples.append({
                    "sample_id": len(detailed_results),
                    "image_path": result["image"],
                    "max_iou": max_sample_iou,
                    "ground_truth": {
                        "has_abnormality": gt_has_abnormality,
                        "bboxes": gt_bboxes,
                        "raw_annotation": result.get("raw_annotation", ""),
                        "extracted_answer": result.get("extracted_answer", "")
                    },
                    "prediction": {
                        "has_abnormality": pred_has_abnormality,
                        "bboxes": pred_bboxes_relative,
                        "raw_prediction": pred_text,
                        "extracted_prediction_answer": result.get("extracted_prediction_answer", "")
                    }
                })
            elif max_sample_iou > 0.7:
                good_samples.append({
                    "sample_id": len(detailed_results),
                    "image_path": result["image"],
                    "max_iou": max_sample_iou,
                    "ground_truth": {
                        "has_abnormality": gt_has_abnormality,
                        "bboxes": gt_bboxes,
                        "raw_annotation": result.get("raw_annotation", ""),
                        "extracted_answer": result.get("extracted_answer", "")
                    },
                    "prediction": {
                        "has_abnormality": pred_has_abnormality,
                        "bboxes": pred_bboxes_relative,
                        "raw_prediction": pred_text,
                        "extracted_prediction_answer": result.get("extracted_prediction_answer", "")
                    }
                })
            elif max_sample_iou > 0.5:
                medium_samples.append({
                    "sample_id": len(detailed_results),
                    "image_path": result["image"],
                    "max_iou": max_sample_iou,
                    "ground_truth": {
                        "has_abnormality": gt_has_abnormality,
                        "bboxes": gt_bboxes,
                        "raw_annotation": result.get("raw_annotation", ""),
                        "extracted_answer": result.get("extracted_answer", "")
                    },
                    "prediction": {
                        "has_abnormality": pred_has_abnormality,
                        "bboxes": pred_bboxes_relative,
                        "raw_prediction": pred_text,
                        "extracted_prediction_answer": result.get("extracted_prediction_answer", "")
                    }
                })
        
        # å¯¹æ¯ä¸ªIoUé˜ˆå€¼è¿›è¡Œè¯„ä¼°
        sample_results = {}
        for iou_th in iou_thresholds:
            eval_result = evaluate_detection_improved(
                pred_bboxes_relative, gt_bboxes, 
                pred_has_abnormality, gt_has_abnormality, 
                iou_th
            )
            
            # æ›´æ–°ç»Ÿè®¡
            iou_metrics[f"iou_{iou_th}"]["total_samples"] += 1
            if eval_result["correct"]:
                iou_metrics[f"iou_{iou_th}"]["correct_samples"] += 1
            
            # æŒ‰æƒ…å†µç»Ÿè®¡
            case = eval_result["case"]
            if case == "true_negative":
                iou_metrics[f"iou_{iou_th}"]["true_negatives"] += 1
            elif case == "false_positive":
                iou_metrics[f"iou_{iou_th}"]["false_positives"] += 1
            elif case == "false_negative":
                iou_metrics[f"iou_{iou_th}"]["false_negatives"] += 1
            elif case == "true_positive":
                iou_metrics[f"iou_{iou_th}"]["true_positives"] += 1
            elif case == "low_iou_or_f1":
                iou_metrics[f"iou_{iou_th}"]["low_iou_or_f1"] += 1
            
            sample_results[f"iou_{iou_th}"] = eval_result
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_result = {
            "image_path": result["image"],
            "ground_truth": {
                "has_abnormality": gt_has_abnormality,
                "bboxes": gt_bboxes,
                "bboxes_relative": gt_bboxes_relative,
                "raw_annotation": result.get("raw_annotation", ""),
                "extracted_answer": result.get("extracted_answer", "")
            },
            "prediction": {
                "has_abnormality": pred_has_abnormality,
                "bboxes": pred_bboxes_relative,
                "bboxes_relative": pred_bboxes_relative,
                "raw_prediction": pred_text,
                "extracted_prediction_answer": result.get("extracted_prediction_answer", "")
            },
            "evaluation": sample_results,
            "max_sample_iou": max_sample_iou  # æ·»åŠ è¯¥æ ·æœ¬çš„æœ€å¤§IoU
        }
        detailed_results.append(detailed_result)
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    final_metrics = {}
    
    # æ·»åŠ é«˜IoUæ ·æœ¬ç»Ÿè®¡ï¼ˆç”¨äºè®ºæ–‡å±•ç¤ºï¼‰
    final_metrics["high_iou_samples"] = {
        "perfect_samples": {
            "count": len(perfect_samples),
            "samples": perfect_samples[:10]  # åªä¿å­˜å‰10ä¸ªå®Œç¾æ ·æœ¬
        },
        "good_samples": {
            "count": len(good_samples),
            "samples": good_samples[:15]  # ä¿å­˜å‰15ä¸ªå¥½æ ·æœ¬
        },
        "medium_samples": {
            "count": len(medium_samples),
            "samples": medium_samples[:20]  # ä¿å­˜å‰20ä¸ªä¸­ç­‰æ ·æœ¬
        }
    }
    
    # æ·»åŠ "çœŸå®æœ‰æ¡†ä¸”é¢„æµ‹æœ‰æ¡†"æƒ…å†µçš„IoUç»Ÿè®¡
    if both_has_bbox_ious:
        final_metrics["both_has_bbox_iou_statistics"] = {
            "mean_iou": np.mean(both_has_bbox_ious),
            "max_iou": np.max(both_has_bbox_ious),
            "min_iou": np.min(both_has_bbox_ious),
            "median_iou": np.median(both_has_bbox_ious),
            "std_iou": np.std(both_has_bbox_ious),
            "total_iou_pairs": len(both_has_bbox_ious),
            "samples_with_both_bbox": len([r for r in detailed_results if r["ground_truth"]["has_abnormality"] and r["prediction"]["has_abnormality"] and len(r["ground_truth"]["bboxes"]) > 0 and len(r["prediction"]["bboxes"]) > 0])
        }
    else:
        final_metrics["both_has_bbox_iou_statistics"] = {
            "mean_iou": 0.0,
            "max_iou": 0.0,
            "min_iou": 0.0,
            "median_iou": 0.0,
            "std_iou": 0.0,
            "total_iou_pairs": 0,
            "samples_with_both_bbox": 0
        }
    
    # æ·»åŠ å…¨å±€IoUç»Ÿè®¡ï¼ˆåŒ…æ‹¬æ‰€æœ‰æœ‰æ¡†æ ·æœ¬çš„IoUï¼‰
    if all_ious:
        final_metrics["global_iou_statistics"] = {
            "mean_iou": np.mean(all_ious),
            "max_iou": np.max(all_ious),
            "min_iou": np.min(all_ious),
            "median_iou": np.median(all_ious),
            "std_iou": np.std(all_ious),
            "total_iou_values": len(all_ious),
            "samples_with_bbox": samples_with_bbox,
            "total_samples": len(results)
        }
    else:
        final_metrics["global_iou_statistics"] = {
            "mean_iou": 0.0,
            "max_iou": 0.0,
            "min_iou": 0.0,
            "median_iou": 0.0,
            "std_iou": 0.0,
            "total_iou_values": 0,
            "samples_with_bbox": 0,
            "total_samples": len(results)
        }
    
    for iou_th in iou_thresholds:
        key = f"iou_threshold_{iou_th}"
        metrics = iou_metrics[f"iou_{iou_th}"]
        
        total = metrics["total_samples"]
        correct = metrics["correct_samples"]
        accuracy = correct / total if total > 0 else 0.0
        
        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
        tp = metrics["true_positives"]
        fp = metrics["false_positives"]
        fn = metrics["false_negatives"]
        tn = metrics["true_negatives"]
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        final_metrics[key] = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "true_positives": tp,
            "false_positives": fp,
            "false_negatives": fn,
            "true_negatives": tn,
            "low_iou_or_f1": metrics["low_iou_or_f1"],
            "total_samples": total,
            "correct_samples": correct
        }
    
    final_metrics["detailed_results"] = detailed_results
    
    return final_metrics

def main():
    # è®¾ç½®GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = "3"
    
    parser = argparse.ArgumentParser(description="æ”¹è¿›çš„X-ray Groundingä»»åŠ¡è¯„ä¼°")
    parser.add_argument("--model_path", type=str, 
                       default="/data0/zhuoxu/yihong/code/LLaMA-Factory-main/saves/Qwen2-VL-7B-Instruct/full/train_grounding_merger_llm_use_cot_5e-6_ep3_stage2",
                       help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_data", type=str, 
                       default="/data0/zhuoxu/yihong/code/EasyR1-main/eval_data/test_grounding_balanced.json",
                       help="æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, 
                       default="/data0/zhuoxu/yihong/code/LLaMA-Factory-main/task_grounding_result_test_balanced_improved_test_train_grounding_merger_llm_use_cot_5e-6_ep3_stage2",
                       help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--iou_thresholds", type=float, nargs="+", 
                       default=[0.3, 0.5, 0.7],
                       help="IoUé˜ˆå€¼åˆ—è¡¨")
    parser.add_argument("--max_samples", type=int, help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°é‡")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("æ”¹è¿›çš„X-ray Groundingä»»åŠ¡è¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"åŠ è½½æµ‹è¯•æ•°æ®: {args.test_data}")
    test_data = load_test_data_improved(args.test_data)
    
    if args.max_samples:
        test_data = test_data[:args.max_samples]
        print(f"é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ä¸º: {args.max_samples}")
    
    print(f"æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_data)}")
    
    # è¿è¡Œæ¨ç†
    print(f"ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†: {args.model_path}")
    inference_output = os.path.join(args.output_dir, "inference_results.json")
    results = run_inference_improved(args.model_path, test_data, inference_output)
    
    # è¯„ä¼°ç»“æœ
    print("è¯„ä¼°æ£€æµ‹æ€§èƒ½...")
    print("ğŸ“Š å°†å®æ—¶æ˜¾ç¤ºmean_iouç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯10ä¸ªæ ·æœ¬æ›´æ–°ä¸€æ¬¡ï¼‰")
    metrics = evaluate_results_improved(results, args.iou_thresholds)
    
    # æ‰“å°ç»“æœ
    print("\nğŸ¯ === æ”¹è¿›çš„è¯„ä¼°ç»“æœ ===")
    print("-" * 80)
    
    # æ‰“å°é«˜IoUæ ·æœ¬ç»Ÿè®¡ï¼ˆç”¨äºè®ºæ–‡å±•ç¤ºï¼‰
    if "high_iou_samples" in metrics:
        high_iou_stats = metrics["high_iou_samples"]
        print(f"\nğŸ† é«˜IoUæ ·æœ¬ç»Ÿè®¡ï¼ˆç”¨äºè®ºæ–‡å±•ç¤ºï¼‰:")
        print(f"  å®Œç¾æ ·æœ¬ (IoU > 0.9): {high_iou_stats['perfect_samples']['count']} ä¸ª")
        print(f"  ä¼˜ç§€æ ·æœ¬ (IoU > 0.7): {high_iou_stats['good_samples']['count']} ä¸ª")
        print(f"  è‰¯å¥½æ ·æœ¬ (IoU > 0.5): {high_iou_stats['medium_samples']['count']} ä¸ª")
        
        # å±•ç¤ºä¸€äº›é«˜IoUæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        if high_iou_stats['perfect_samples']['samples']:
            print(f"\nğŸ¯ å®Œç¾æ ·æœ¬ç¤ºä¾‹ (IoU > 0.9):")
            for i, sample in enumerate(high_iou_stats['perfect_samples']['samples'][:3]):
                print(f"  æ ·æœ¬ {i+1}: IoU = {sample['max_iou']:.4f}")
                print(f"    å›¾ç‰‡: {sample['image_path']}")
                print(f"    çœŸå®æ ‡æ³¨: {sample['ground_truth']['extracted_answer'][:100]}...")
                print(f"    é¢„æµ‹æå–: {sample['prediction'].get('extracted_prediction_answer', 'N/A')[:100]}...")
        
        if high_iou_stats['good_samples']['samples']:
            print(f"\nğŸ‘ ä¼˜ç§€æ ·æœ¬ç¤ºä¾‹ (IoU > 0.7):")
            for i, sample in enumerate(high_iou_stats['good_samples']['samples'][:2]):
                print(f"  æ ·æœ¬ {i+1}: IoU = {sample['max_iou']:.4f}")
                print(f"    å›¾ç‰‡: {sample['image_path']}")
                print(f"    çœŸå®æ ‡æ³¨: {sample['ground_truth']['extracted_answer'][:100]}...")
                print(f"    é¢„æµ‹æå–: {sample['prediction'].get('extracted_prediction_answer', 'N/A')[:100]}...")
    
    # æ‰“å°å…¨å±€IoUç»Ÿè®¡ï¼ˆæ‰€æœ‰æœ‰æ¡†æ ·æœ¬çš„IoUï¼‰
    if "global_iou_statistics" in metrics:
        global_iou_stats = metrics["global_iou_statistics"]
        print(f"\nğŸŒ å…¨å±€IoUç»Ÿè®¡ (æ‰€æœ‰æœ‰æ¡†æ ·æœ¬):")
        print(f"  æ€»æ ·æœ¬æ•°: {global_iou_stats['total_samples']}")
        print(f"  æœ‰æ¡†æ ·æœ¬æ•°: {global_iou_stats['samples_with_bbox']}")
        print(f"  æ€»IoUå€¼æ•°: {global_iou_stats['total_iou_values']}")
        print(f"  å¹³å‡IoU: {global_iou_stats['mean_iou']:.4f}")
        print(f"  æœ€å¤§IoU: {global_iou_stats['max_iou']:.4f}")
        print(f"  æœ€å°IoU: {global_iou_stats['min_iou']:.4f}")
        print(f"  ä¸­ä½IoU: {global_iou_stats['median_iou']:.4f}")
        print(f"  IoUæ ‡å‡†å·®: {global_iou_stats['std_iou']:.4f}")
    
    # æ‰“å°"çœŸå®æœ‰æ¡†ä¸”é¢„æµ‹æœ‰æ¡†"æƒ…å†µçš„IoUç»Ÿè®¡
    if "both_has_bbox_iou_statistics" in metrics:
        bbox_iou_stats = metrics["both_has_bbox_iou_statistics"]
        print(f"\nğŸ“Š çœŸå®æœ‰æ¡†ä¸”é¢„æµ‹æœ‰æ¡†æƒ…å†µçš„IoUç»Ÿè®¡:")
        print(f"  æ ·æœ¬æ•°: {bbox_iou_stats['samples_with_both_bbox']}")
        print(f"  å¹³å‡IoU: {bbox_iou_stats['mean_iou']:.4f}")
        print(f"  æœ€å¤§IoU: {bbox_iou_stats['max_iou']:.4f}")
        print(f"  æœ€å°IoU: {bbox_iou_stats['min_iou']:.4f}")
        print(f"  ä¸­ä½IoU: {bbox_iou_stats['median_iou']:.4f}")
        print(f"  IoUæ ‡å‡†å·®: {bbox_iou_stats['std_iou']:.4f}")
        print(f"  æ€»IoUå¯¹æ•°: {bbox_iou_stats['total_iou_pairs']}")
    
    # æŒ‰IoUé˜ˆå€¼æ‰“å°ç»“æœ
    for iou_th in args.iou_thresholds:
        key = f"iou_threshold_{iou_th}"
        threshold_metrics = metrics[key]
        
        print(f"\nğŸ” IoUé˜ˆå€¼ {iou_th}:")
        print(f"  æ€»æ ·æœ¬æ•°: {threshold_metrics['total_samples']}")
        print(f"  æ­£ç¡®æ ·æœ¬æ•°: {threshold_metrics['correct_samples']}")
        print(f"  å‡†ç¡®ç‡: {threshold_metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {threshold_metrics['precision']:.4f}")
        print(f"  å¬å›ç‡: {threshold_metrics['recall']:.4f}")
        print(f"  F1åˆ†æ•°: {threshold_metrics['f1']:.4f}")
        print(f"  çœŸé˜³æ€§(TP): {threshold_metrics['true_positives']}")
        print(f"  å‡é˜³æ€§(FP): {threshold_metrics['false_positives']}")
        print(f"  çœŸé˜´æ€§(TN): {threshold_metrics['true_negatives']}")
        print(f"  å‡é˜´æ€§(FN): {threshold_metrics['false_negatives']}")
        print(f"  ä½IoU/F1: {threshold_metrics['low_iou_or_f1']}")
    
    # æ‰“å°æœ€ç»ˆçš„mean_iouæ€»ç»“
    if "global_iou_statistics" in metrics:
        final_mean_iou = metrics["global_iou_statistics"]["mean_iou"]
        print(f"\nğŸ¯ === æœ€ç»ˆMean IoUæ€»ç»“ ===")
        print(f"  æœ€ç»ˆå¹³å‡IoU: {final_mean_iou:.4f}")
        print(f"  åŸºäº {metrics['global_iou_statistics']['total_iou_values']} ä¸ªIoUå€¼è®¡ç®—")
        print(f"  æ¥è‡ª {metrics['global_iou_statistics']['samples_with_bbox']} ä¸ªæœ‰æ¡†æ ·æœ¬")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    metrics_file = os.path.join(args.output_dir, "evaluation_metrics.json")
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_file = os.path.join(args.output_dir, "detailed_results.json")
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump(metrics["detailed_results"], f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜é«˜IoUæ ·æœ¬ï¼ˆç”¨äºè®ºæ–‡å±•ç¤ºï¼‰
    high_iou_file = os.path.join(args.output_dir, "high_iou_samples_for_paper.json")
    with open(high_iou_file, 'w', encoding='utf-8') as f:
        json.dump(metrics["high_iou_samples"], f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆé”™è¯¯æ¡ˆä¾‹åˆ†æ
    error_analysis = analyze_errors(metrics["detailed_results"])
    error_file = os.path.join(args.output_dir, "error_analysis.json")
    with open(error_file, 'w', encoding='utf-8') as f:
        json.dump(error_analysis, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
    print(f"ğŸ“Š æ¨ç†ç»“æœ: {inference_output}")
    print(f"ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡: {metrics_file}")
    print(f"ğŸ“‹ è¯¦ç»†ç»“æœ: {detailed_file}")
    print(f"ğŸ† é«˜IoUæ ·æœ¬: {high_iou_file}")
    print(f"âŒ é”™è¯¯åˆ†æ: {error_file}")

def analyze_errors(detailed_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    åˆ†æé”™è¯¯æ¡ˆä¾‹
    """
    error_analysis = {
        "false_positives": [],
        "false_negatives": [],
        "low_iou_cases": [],
        "bbox_mismatch": []
    }
    
    for i, result in enumerate(detailed_results):
        # æ£€æŸ¥å„ç§é”™è¯¯æƒ…å†µ
        for iou_th in [0.3, 0.5, 0.7]:
            eval_key = f"iou_{iou_th}"
            if eval_key in result["evaluation"]:
                eval_result = result["evaluation"][eval_key]
                
                if eval_result["case"] == "false_positive":
                    error_analysis["false_positives"].append({
                        "sample_id": i,
                        "image_path": result["image_path"],
                        "ground_truth": result["ground_truth"],
                        "prediction": result["prediction"],
                        "iou_threshold": iou_th
                    })
                elif eval_result["case"] == "false_negative":
                    error_analysis["false_negatives"].append({
                        "sample_id": i,
                        "image_path": result["image_path"],
                        "ground_truth": result["ground_truth"],
                        "prediction": result["prediction"],
                        "iou_threshold": iou_th
                    })
                elif eval_result["case"] == "low_iou_or_f1":
                    error_analysis["low_iou_cases"].append({
                        "sample_id": i,
                        "image_path": result["image_path"],
                        "ground_truth": result["ground_truth"],
                        "prediction": result["prediction"],
                        "iou_threshold": iou_th,
                        "max_iou": eval_result.get("max_iou", 0.0),
                        "f1": eval_result.get("f1", 0.0)
                    })
                elif eval_result["case"] == "bbox_mismatch":
                    error_analysis["bbox_mismatch"].append({
                        "sample_id": i,
                        "image_path": result["image_path"],
                        "ground_truth": result["ground_truth"],
                        "prediction": result["prediction"],
                        "iou_threshold": iou_th
                    })
    
    # ç»Ÿè®¡é”™è¯¯æ•°é‡ - ä¿®å¤ç±»å‹é”™è¯¯
    error_analysis["error_statistics"] = {
        "total_samples": len(detailed_results),
        "false_positives_count": len(error_analysis["false_positives"]),
        "false_negatives_count": len(error_analysis["false_negatives"]),
        "low_iou_cases_count": len(error_analysis["low_iou_cases"]),
        "bbox_mismatch_count": len(error_analysis["bbox_mismatch"])
    }
    
    return error_analysis

if __name__ == "__main__":
    main()