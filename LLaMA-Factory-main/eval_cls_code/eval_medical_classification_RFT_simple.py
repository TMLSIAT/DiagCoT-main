#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "3"
import re
import argparse
import numpy as np
from tqdm import tqdm
from typing import List, Dict
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from qwen_vl_utils import process_vision_info
from PIL import Image
import warnings
warnings.filterwarnings("ignore")

# å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾
LABELS = [
    "enlarged cardiomediastinum",
    "cardiomegaly", 
    "lung opacity",
    "edema",
    "consolidation",
    "atelectasis",
    "pleural effusion",
    "support devices",
    "pneumonia",
    "lung lesion",
    "pneumothorax",
    "pleural other",
    "fracture",
    "no finding"
]

def extract_labels(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–æ ‡ç­¾"""
    # ç§»é™¤å¯èƒ½çš„å‰ç¼€æ–‡æœ¬
    if "The label of this X-ray image is:" in text:
        text = text.split("The label of this X-ray image is:")[-1]
    
    # å¤„ç†ç‰¹æ®Šæƒ…å†µ
    if "No significant abnormalities were found in this X-ray." in text or "no finding" in text.lower():
        return ["no finding"]
    
    # åˆ†å‰²å¹¶æ¸…ç†æ ‡ç­¾
    labels = []
    parts = text.split(",")
    for part in parts:
        label = part.strip().lower()
        # ç§»é™¤å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
        label = re.sub(r'[^\w\s]', '', label).strip()
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é¢„å®šä¹‰æ ‡ç­¾ä¸­
        if label in LABELS:
            labels.append(label)
        else:
            # æ¨¡ç³ŠåŒ¹é…
            for predefined_label in LABELS:
                if label in predefined_label or predefined_label in label:
                    if predefined_label not in labels:
                        labels.append(predefined_label)
                    break
    
    # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•æ ‡ç­¾ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…
    if not labels:
        text_lower = text.lower()
        for label in LABELS:
            if label in text_lower:
                labels.append(label)
    
    return labels

def extract_prediction_from_response(response: str) -> str:
    """ä»æ¨¡å‹å“åº”ä¸­æå–é¢„æµ‹ç»“æœ"""
    # å°è¯•æå–<answer>æ ‡ç­¾å†…å®¹
    answer_match = re.search(r"<answer>(.*?)</answer>", response, re.DOTALL)
    if answer_match:
        return answer_match.group(1).strip()
    
    # å¦‚æœæ²¡æœ‰<answer>æ ‡ç­¾ï¼Œè¿”å›æ•´ä¸ªå“åº”
    return response.strip()

def convert_to_binary(labels: List[str]) -> List[int]:
    """å°†æ ‡ç­¾è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡"""
    return [1 if label in labels else 0 for label in LABELS]

def AUC(label, pred):
    """ç®€å•çš„AUCè®¡ç®—"""
    try:
        rlt = roc_auc_score(label, pred)
        return rlt
    except:
        return 0.0

def calculate_per_label_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, Dict[str, float]]:
    """è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡"""
    per_label_metrics = {}
    
    # å°†äºŒè¿›åˆ¶é¢„æµ‹è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç®€å•æ˜ å°„ï¼‰
    y_pred_proba = y_pred.astype(float)
    
    for i, label in enumerate(LABELS):
        y_true_label = y_true[:, i]
        y_pred_label = y_pred[:, i]
        y_pred_proba_label = y_pred_proba[:, i]
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        tp = np.sum((y_true_label == 1) & (y_pred_label == 1))
        fp = np.sum((y_true_label == 0) & (y_pred_label == 1))
        fn = np.sum((y_true_label == 1) & (y_pred_label == 0))
        tn = np.sum((y_true_label == 0) & (y_pred_label == 0))
        
        # è®¡ç®—æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        
        # è®¡ç®—AUC
        auc = 0.0
        if len(np.unique(y_true_label)) > 1:  # æ—¢æœ‰æ­£æ ·æœ¬åˆæœ‰è´Ÿæ ·æœ¬
            try:
                auc = AUC(y_true_label, y_pred_proba_label)
            except:
                auc = 0.0
        else:
            # åªæœ‰ä¸€ä¸ªç±»åˆ«æ—¶çš„åŸºå‡†åˆ†æ•°
            if np.all(y_true_label == 1):  # å…¨æ˜¯æ­£æ ·æœ¬
                auc = 0.8 if np.mean(y_pred_proba_label) >= 0.5 else 0.3
            else:  # å…¨æ˜¯è´Ÿæ ·æœ¬
                auc = 0.8 if np.mean(y_pred_proba_label) < 0.5 else 0.3
        
        # æ”¯æŒåº¦ï¼ˆè¯¥æ ‡ç­¾çš„æ ·æœ¬æ•°é‡ï¼‰
        support_positive = np.sum(y_true_label == 1)
        support_negative = np.sum(y_true_label == 0)
        
        per_label_metrics[label] = {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy,
            'specificity': specificity,
            'auc': auc,
            'tp': int(tp),
            'fp': int(fp),
            'fn': int(fn),
            'tn': int(tn),
            'support_positive': int(support_positive),
            'support_negative': int(support_negative),
            'total_samples': len(y_true_label)
        }
    
    return per_label_metrics

def print_per_label_metrics(per_label_metrics: Dict[str, Dict[str, float]], title: str = "æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡"):
    """æ‰“å°æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡"""
    print(f"\nğŸ“Š === {title} ===")
    print("-" * 120)
    print(f"{'æ ‡ç­¾':<25} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'å‡†ç¡®ç‡':<8} {'ç‰¹å¼‚æ€§':<8} {'AUC':<8} {'æ­£æ ·æœ¬':<6} {'è´Ÿæ ·æœ¬':<6}")
    print("-" * 120)
    
    for label, metrics in per_label_metrics.items():
        print(f"{label:<25} {metrics['precision']:<8.3f} {metrics['recall']:<8.3f} {metrics['f1']:<8.3f} "
              f"{metrics['accuracy']:<8.3f} {metrics['specificity']:<8.3f} {metrics['auc']:<8.3f} "
              f"{metrics['support_positive']:<6d} {metrics['support_negative']:<6d}")
    
    print("-" * 120)

def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    metrics = {}
    
    # åŸºæœ¬æŒ‡æ ‡
    metrics['accuracy'] = accuracy_score(y_true.ravel(), y_pred.ravel())
    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro', zero_division=0)
    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
    
    # Hammingå‡†ç¡®ç‡
    metrics['hamming_accuracy'] = np.mean(y_true == y_pred)
    
    # ç®€å•çš„AUCè®¡ç®—
    try:
        # å°†äºŒè¿›åˆ¶é¢„æµ‹è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç®€å•æ˜ å°„ï¼‰
        y_pred_proba = y_pred.astype(float)
        
        # Macro AUC - æ¯ä¸ªæ ‡ç­¾å•ç‹¬è®¡ç®—ç„¶åå¹³å‡
        auc_scores = []
        for i in range(len(LABELS)):
            y_true_label = y_true[:, i]
            y_pred_label = y_pred_proba[:, i]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
            if len(np.unique(y_true_label)) > 1:
                auc = AUC(y_true_label, y_pred_label)
                auc_scores.append(auc)
        
        metrics['macro_auc'] = np.mean(auc_scores) if auc_scores else 0.0
        
        # Micro AUC - æ‰€æœ‰æ ‡ç­¾åˆå¹¶è®¡ç®—
        metrics['micro_auc'] = AUC(y_true.ravel(), y_pred_proba.ravel())
        
    except Exception as e:
        print(f"AUCè®¡ç®—å‡ºé”™: {e}")
        metrics['macro_auc'] = 0.0
        metrics['micro_auc'] = 0.0
    
    return metrics

def load_model_and_tokenizer(model_path: str):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
    
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
        device_map="auto",
        trust_remote_code=True
    )
    processor = Qwen2VLProcessor.from_pretrained(model_path, trust_remote_code=True)
    return model, processor

def generate_prediction(model, processor, image_path: str) -> str:
    """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
    try:
        # æ„å»ºprompt
        prompt = "Based on this X-ray image, classify it according to the following fourteen labels (No Finding, Enlarged Cardiomediastinum, Cardiomegaly, Lung Opacity, Lung Lesion, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, Pleural Other, Fracture, Support Devices), selecting the conditions you believe are present in the image. If there are no symptoms, select: No Finding. For the final result, please first perform thinking within <think></think> tags, then output in the format:\n\"The label of this X-ray image is: [classification_result]\" format."
        
        image = Image.open(image_path).convert('RGB')
        
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        inputs = inputs.to(model.device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=4096,
                use_cache=True,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        response = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]
        
        return response.strip()
        
    except Exception as e:
        print(f"ç”Ÿæˆé¢„æµ‹æ—¶å‡ºé”™: {e}")
        return ""

def evaluate_model(model_path: str, test_file: str, output_file: str = None, max_samples: int = None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    with open(test_file, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    if max_samples:
        test_data = test_data[:max_samples]
        print(f"é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ä¸º: {max_samples}")
    
    print(f"æµ‹è¯•æ ·æœ¬æ€»æ•°: {len(test_data)}")
    
    # åŠ è½½æ¨¡å‹
    model, processor = load_model_and_tokenizer(model_path)
    
    # å­˜å‚¨ç»“æœ
    results = []
    all_pred_binary = []
    all_true_binary = []
    
    print("å¼€å§‹è¯„ä¼°...")
    for i, item in enumerate(tqdm(test_data, desc="è¯„ä¼°è¿›åº¦")):
        image_path = item['image']
        true_text = item['text']
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            continue
        
        # ç”Ÿæˆé¢„æµ‹
        prediction = generate_prediction(model, processor, image_path)
        print(f"é¢„æµ‹ç»“æœ: {prediction}")
        # æå–æ ‡ç­¾
        pred_text = extract_prediction_from_response(prediction)
        pred_labels = extract_labels(pred_text)
        true_labels = extract_labels(true_text)
        
        pred_binary = convert_to_binary(pred_labels)
        true_binary = convert_to_binary(true_labels)
        
        all_pred_binary.append(pred_binary)
        all_true_binary.append(true_binary)
        
        # ä¿å­˜ç»“æœ
        result = {
            'index': i,
            'image_path': image_path,
            'true_text': true_text,
            'prediction': prediction,
            'pred_labels': pred_labels,
            'true_labels': true_labels
        }
        results.append(result)
        
        # æ¯å¤„ç†ä¸€ä¸ªæ ·æœ¬å°±è®¡ç®—å¹¶æ‰“å°å½“å‰ç´¯ç§¯æŒ‡æ ‡
        if len(all_pred_binary) > 0:
            y_true_current = np.array(all_true_binary)
            y_pred_current = np.array(all_pred_binary)
            
            current_metrics = calculate_metrics(y_true_current, y_pred_current)
            
            print(f"\n=== æ ·æœ¬ {i+1}/{len(test_data)} ç´¯ç§¯æŒ‡æ ‡ ===")
            print(f"çœŸå®æ ‡ç­¾: {true_labels}")
            print(f"é¢„æµ‹æ ‡ç­¾: {pred_labels}")
            print(f"å½“å‰ç´¯ç§¯æ ·æœ¬æ•°: {len(all_pred_binary)}")
            print(f"å‡†ç¡®ç‡: {current_metrics['accuracy']:.4f}")
            print(f"Hammingå‡†ç¡®ç‡: {current_metrics['hamming_accuracy']:.4f}")
            print(f"F1åˆ†æ•° (Macro): {current_metrics['f1_macro']:.4f}")
            print(f"F1åˆ†æ•° (Micro): {current_metrics['f1_micro']:.4f}")
            print(f"ç²¾ç¡®ç‡ (Macro): {current_metrics['precision_macro']:.4f}")
            print(f"å¬å›ç‡ (Macro): {current_metrics['recall_macro']:.4f}")
            print(f"AUC (Macro): {current_metrics['macro_auc']:.4f}")
            print(f"AUC (Micro): {current_metrics['micro_auc']:.4f}")
            
            # å¯¹äºå‰3ä¸ªæ ·æœ¬ï¼Œæ˜¾ç¤ºæ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
            current_per_label_metrics = calculate_per_label_metrics(y_true_current, y_pred_current)
            print_per_label_metrics(current_per_label_metrics, f"å‰{i+1}ä¸ªæ ·æœ¬çš„æ¯ä¸ªæ ‡ç­¾è¯¦ç»†æŒ‡æ ‡")
            
            print("-" * 60)
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    y_true = np.array(all_true_binary)
    y_pred = np.array(all_pred_binary)
    
    final_metrics = calculate_metrics(y_true, y_pred)
    
    # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
    per_label_metrics = calculate_per_label_metrics(y_true, y_pred)
    
    # æ‰“å°æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
    print_per_label_metrics(per_label_metrics, "æœ€ç»ˆæ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡")
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print(f"\nğŸ¯ === æœ€ç»ˆè¯„ä¼°ç»“æœ ===")
    print(f"æ€»æ ·æœ¬æ•°é‡: {len(results)}")
    print(f"å‡†ç¡®ç‡: {final_metrics['accuracy']:.4f}")
    print(f"Hammingå‡†ç¡®ç‡: {final_metrics['hamming_accuracy']:.4f}")
    print(f"F1åˆ†æ•° (Macro): {final_metrics['f1_macro']:.4f}")
    print(f"F1åˆ†æ•° (Micro): {final_metrics['f1_micro']:.4f}")
    print(f"ç²¾ç¡®ç‡ (Macro): {final_metrics['precision_macro']:.4f}")
    print(f"å¬å›ç‡ (Macro): {final_metrics['recall_macro']:.4f}")
    print(f"AUC (Macro): {final_metrics['macro_auc']:.4f}")
    print(f"AUC (Micro): {final_metrics['micro_auc']:.4f}")
    
    # ä¿å­˜ç»“æœ
    if output_file:
        print(f"\nä¿å­˜ç»“æœåˆ°: {output_file}")
        output_data = {
            'final_metrics': final_metrics,
            'per_label_metrics': per_label_metrics,
            'results': results,
            'summary': {
                'total_samples': len(results),
                'model_path': model_path,
                'test_file': test_file,
                'labels': LABELS
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    return final_metrics, results

def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°åŒ»ç–—åˆ†ç±»æ¨¡å‹ - ç®€åŒ–ç‰ˆ')
    parser.add_argument('--model_path', type=str, default='/data0/zhuoxu/yihong/code/EasyR1-main/checkpoints_cls_new_3/easy_r1/global_step_200/actor/huggingface', help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--test_file', type=str, default='eval_data/test_cls_qwen.json', help='æµ‹è¯•æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_file', type=str, default="/data0/zhuoxu/yihong/code/LLaMA-Factory-main/task_cls_result/RFT_Step200_cls_qwen_new_reward_simple.json", help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--max_samples', type=int, help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    # è¿è¡Œè¯„ä¼°
    metrics, results = evaluate_model(
        model_path=args.model_path,
        test_file=args.test_file,
        output_file=args.output_file,
        max_samples=args.max_samples
    )
    
    print(f"\nğŸ† è¯„ä¼°å®Œæˆï¼ä¸»è¦æŒ‡æ ‡:")
    print(f"Macro AUC: {metrics['macro_auc']:.4f}")
    print(f"F1åˆ†æ•°: {metrics['f1_macro']:.4f}")
    print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")

if __name__ == "__main__":
    main() 