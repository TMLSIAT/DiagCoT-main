#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨VLLMéƒ¨ç½²çš„72Bæ¨¡å‹è¿›è¡ŒåŒ»å­¦å½±åƒåˆ†ç±»è¯„ä¼°
é€šè¿‡OpenAI APIæ¥å£è°ƒç”¨æœ¬åœ°VLLMæœåŠ¡
ç»“åˆåˆ†ç±»ä»»åŠ¡è¯„ä¼°å’ŒVLLM APIè°ƒç”¨åŠŸèƒ½
"""

import json
import os
import re
import base64
import argparse
import numpy as np
from tqdm import tqdm
from typing import List, Dict
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score
from openai import OpenAI
from retrying import retry
import warnings
warnings.filterwarnings("ignore")

# å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾
LABELS = [
    "enlarged cardiomediastinum",
    "cardiomegaly", 
    "lung opacity",
    "edema",
    "consolidation",
    "atelectasis",
    "pleural effusion",
    "support devices",
    "pneumonia",
    "lung lesion",
    "pneumothorax",
    "pleural other",
    "fracture",
    "no finding"
]

def encode_image(image_path):
    """
    å°†å›¾åƒæ–‡ä»¶ç¼–ç ä¸ºbase64æ ¼å¼
    
    Args:
        image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
        
    Returns:
        str: base64ç¼–ç çš„å›¾åƒæ•°æ®
    """
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

class VLLMClassifier:
    """
    ä½¿ç”¨VLLM APIçš„åŒ»å­¦å½±åƒåˆ†ç±»å™¨
    """
    
    def __init__(self, model_name, api_key="qwen-abc123", api_url="http://127.0.0.1:8000/v1"):
        """
        åˆå§‹åŒ–VLLMåˆ†ç±»å™¨
        
        Args:
            model_name (str): æ¨¡å‹åç§°
            api_key (str): APIå¯†é’¥
            api_url (str): APIæœåŠ¡åœ°å€
        """
        self.model_name = model_name
        self.api_key = api_key
        self.api_url = api_url
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.api_url,
        )
        print(f"åˆå§‹åŒ–VLLMåˆ†ç±»å™¨ï¼Œæ¨¡å‹: {model_name}, APIåœ°å€: {api_url}")

    def _construct_messages(self, image_path):
        """
        æ„é€ å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯æ ¼å¼
        
        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            list: æ ¼å¼åŒ–çš„æ¶ˆæ¯åˆ—è¡¨
        """
        # å°†å›¾åƒç¼–ç ä¸ºbase64
        base64_image = encode_image(image_path)
        
        return [{
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }
                },
                {
                    "type": "text",
                    "text": "Based on this X-ray image, classify it according to the following fourteen labels (No Finding, Enlarged Cardiomediastinum, Cardiomegaly, Lung Opacity, Lung Lesion, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, Pleural Other, Fracture, Support Devices), selecting the conditions you believe are present in the image. If there are no symptoms, select: No Finding. Output the thinking process in <think> </think>, and output the final classification result within the <answer> </answer>. Following \"<think></think>\n<answer></answer> \" format. The label of this X-ray image is: [classification_result]"
                }
            ]
        }]

    @retry(wait_fixed=3000, stop_max_attempt_number=3)
    def _retry_call(self, messages):
        """
        å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨
        
        Args:
            messages (list): æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„å“åº”
        """
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            # max_tokens=4096,
        )
        return response.choices[0].message.content

    def classify(self, image_path):
        """
        å¯¹åŒ»å­¦å½±åƒè¿›è¡Œåˆ†ç±»
        
        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: åˆ†ç±»ç»“æœ
        """
        try:
            # æ„é€ æ¶ˆæ¯
            messages = self._construct_messages(image_path)
            
            # è°ƒç”¨APIè¿›è¡Œåˆ†ç±»
            response = self._retry_call(messages)
            
            return response if response else ""
            
        except Exception as e:
            print(f"åˆ†ç±»æ—¶å‡ºé”™ {image_path}: {str(e)}")
            return ""

def extract_answer_content(raw_prediction):
    """
    ç›´æ¥æå– <answer> æ ‡ç­¾å†…çš„å®Œæ•´å†…å®¹ï¼ˆä¿ç•™åŸå§‹æ ¼å¼ï¼‰
    å…¼å®¹æ ‡ç­¾å¤§å°å†™ã€å‰åç©ºæ ¼ç­‰æƒ…å†µ
    """
    # ç»Ÿä¸€å¤„ç†è¾“å…¥æ ¼å¼
    text = raw_prediction[0] if isinstance(raw_prediction, list) else str(raw_prediction)
    
    # åŒ¹é…ä»»æ„å¤§å°å†™å’Œç©ºæ ¼çš„æ ‡ç­¾
    match = re.search(r'<\s*answer\s*>([\s\S]*?)<\s*/\s*answer\s*>', text, re.IGNORECASE)
    
    if match:
        # æå–å†…å®¹å¹¶å»é™¤é¦–å°¾ç©ºç™½
        return match.group(1).strip()
    else:
        print(f"WARNING: No <answer> tag found in:\n{text}")
        return raw_prediction

def extract_labels(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ ‡ç­¾
    
    Args:
        text (str): è¾“å…¥æ–‡æœ¬
        
    Returns:
        List[str]: æå–çš„æ ‡ç­¾åˆ—è¡¨
    """
    # ç§»é™¤å¯èƒ½çš„å‰ç¼€æ–‡æœ¬
    if "The label of this X-ray image is:" in text:
        text = text.split("The label of this X-ray image is:")[-1]
    
    # å¤„ç†ç‰¹æ®Šæƒ…å†µ
    if "No significant abnormalities were found in this X-ray." in text or "no finding" in text.lower():
        return ["no finding"]
    
    # åˆ†å‰²å¹¶æ¸…ç†æ ‡ç­¾
    labels = []
    parts = text.split(",")
    for part in parts:
        label = part.strip().lower()
        # ç§»é™¤å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
        label = re.sub(r'[^\w\s]', '', label).strip()
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é¢„å®šä¹‰æ ‡ç­¾ä¸­
        if label in LABELS:
            labels.append(label)
        else:
            # æ¨¡ç³ŠåŒ¹é…
            for predefined_label in LABELS:
                if label in predefined_label or predefined_label in label:
                    if predefined_label not in labels:
                        labels.append(predefined_label)
                    break
    
    # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•æ ‡ç­¾ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…
    if not labels:
        text_lower = text.lower()
        for label in LABELS:
            if label in text_lower:
                labels.append(label)
    
    return labels

def convert_to_binary(labels: List[str]) -> List[int]:
    """
    å°†æ ‡ç­¾è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡
    
    Args:
        labels (List[str]): æ ‡ç­¾åˆ—è¡¨
        
    Returns:
        List[int]: äºŒè¿›åˆ¶å‘é‡
    """
    return [1 if label in labels else 0 for label in LABELS]

def AUC(label, pred):
    """
    ç®€å•çš„AUCè®¡ç®—
    
    Args:
        label: çœŸå®æ ‡ç­¾
        pred: é¢„æµ‹ç»“æœ
        
    Returns:
        float: AUCåˆ†æ•°
    """
    try:
        rlt = roc_auc_score(label, pred)
        return rlt
    except:
        return 0.0

def calculate_per_label_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, Dict[str, float]]:
    """
    è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
    
    Args:
        y_true (np.ndarray): çœŸå®æ ‡ç­¾
        y_pred (np.ndarray): é¢„æµ‹æ ‡ç­¾
        
    Returns:
        Dict[str, Dict[str, float]]: æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
    """
    per_label_metrics = {}
    
    # å°†äºŒè¿›åˆ¶é¢„æµ‹è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç®€å•æ˜ å°„ï¼‰
    y_pred_proba = y_pred.astype(float)
    
    for i, label in enumerate(LABELS):
        y_true_label = y_true[:, i]
        y_pred_label = y_pred[:, i]
        y_pred_proba_label = y_pred_proba[:, i]
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        tp = np.sum((y_true_label == 1) & (y_pred_label == 1))
        fp = np.sum((y_true_label == 0) & (y_pred_label == 1))
        fn = np.sum((y_true_label == 1) & (y_pred_label == 0))
        tn = np.sum((y_true_label == 0) & (y_pred_label == 0))
        
        # è®¡ç®—æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        
        # è®¡ç®—AUC
        auc = 0.0
        if len(np.unique(y_true_label)) > 1:  # æ—¢æœ‰æ­£æ ·æœ¬åˆæœ‰è´Ÿæ ·æœ¬
            try:
                auc = AUC(y_true_label, y_pred_proba_label)
            except:
                auc = 0.0
        else:
            # åªæœ‰ä¸€ä¸ªç±»åˆ«æ—¶çš„åŸºå‡†åˆ†æ•°
            if np.all(y_true_label == 1):  # å…¨æ˜¯æ­£æ ·æœ¬
                auc = 0.8 if np.mean(y_pred_proba_label) >= 0.5 else 0.3
            else:  # å…¨æ˜¯è´Ÿæ ·æœ¬
                auc = 0.8 if np.mean(y_pred_proba_label) < 0.5 else 0.3
        
        # æ”¯æŒåº¦ï¼ˆè¯¥æ ‡ç­¾çš„æ ·æœ¬æ•°é‡ï¼‰
        support_positive = np.sum(y_true_label == 1)
        support_negative = np.sum(y_true_label == 0)
        
        per_label_metrics[label] = {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy,
            'specificity': specificity,
            'auc': auc,
            'tp': int(tp),
            'fp': int(fp),
            'fn': int(fn),
            'tn': int(tn),
            'support_positive': int(support_positive),
            'support_negative': int(support_negative),
            'total_samples': len(y_true_label)
        }
    
    return per_label_metrics

def print_per_label_metrics(per_label_metrics: Dict[str, Dict[str, float]], title: str = "æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡"):
    """
    æ‰“å°æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
    
    Args:
        per_label_metrics (Dict[str, Dict[str, float]]): æ¯ä¸ªæ ‡ç­¾çš„æŒ‡æ ‡
        title (str): æ ‡é¢˜
    """
    print(f"\nğŸ“Š === {title} ===")
    print("-" * 120)
    print(f"{'æ ‡ç­¾':<25} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'å‡†ç¡®ç‡':<8} {'ç‰¹å¼‚æ€§':<8} {'AUC':<8} {'æ­£æ ·æœ¬':<6} {'è´Ÿæ ·æœ¬':<6}")
    print("-" * 120)
    
    for label, metrics in per_label_metrics.items():
        print(f"{label:<25} {metrics['precision']:<8.3f} {metrics['recall']:<8.3f} {metrics['f1']:<8.3f} "
              f"{metrics['accuracy']:<8.3f} {metrics['specificity']:<8.3f} {metrics['auc']:<8.3f} "
              f"{metrics['support_positive']:<6d} {metrics['support_negative']:<6d}")
    
    print("-" * 120)

def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    
    Args:
        y_true (np.ndarray): çœŸå®æ ‡ç­¾
        y_pred (np.ndarray): é¢„æµ‹æ ‡ç­¾
        
    Returns:
        Dict[str, float]: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    metrics = {}
    
    # åŸºæœ¬æŒ‡æ ‡
    metrics['accuracy'] = accuracy_score(y_true.ravel(), y_pred.ravel())
    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro', zero_division=0)
    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
    
    # Hammingå‡†ç¡®ç‡
    metrics['hamming_accuracy'] = np.mean(y_true == y_pred)
    
    # ç®€å•çš„AUCè®¡ç®—
    try:
        # å°†äºŒè¿›åˆ¶é¢„æµ‹è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç®€å•æ˜ å°„ï¼‰
        y_pred_proba = y_pred.astype(float)
        
        # Macro AUC - æ¯ä¸ªæ ‡ç­¾å•ç‹¬è®¡ç®—ç„¶åå¹³å‡
        auc_scores = []
        for i in range(len(LABELS)):
            y_true_label = y_true[:, i]
            y_pred_label = y_pred_proba[:, i]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
            if len(np.unique(y_true_label)) > 1:
                auc = AUC(y_true_label, y_pred_label)
                auc_scores.append(auc)
        
        metrics['macro_auc'] = np.mean(auc_scores) if auc_scores else 0.0
        
        # Micro AUC - æ‰€æœ‰æ ‡ç­¾åˆå¹¶è®¡ç®—
        metrics['micro_auc'] = AUC(y_true.ravel(), y_pred_proba.ravel())
        
    except Exception as e:
        print(f"AUCè®¡ç®—å‡ºé”™: {e}")
        metrics['macro_auc'] = 0.0
        metrics['micro_auc'] = 0.0
    
    return metrics

def evaluate_model_vllm(model_name: str, test_file: str, api_url: str = "http://127.0.0.1:8000/v1", 
                       output_file: str = None, max_samples: int = None):
    """
    ä½¿ç”¨VLLM APIè¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model_name (str): æ¨¡å‹åç§°
        test_file (str): æµ‹è¯•æ–‡ä»¶è·¯å¾„
        api_url (str): VLLM APIæœåŠ¡åœ°å€
        output_file (str): è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„
        max_samples (int): æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°é‡
        
    Returns:
        tuple: (æœ€ç»ˆæŒ‡æ ‡, ç»“æœåˆ—è¡¨)
    """
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    with open(test_file, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    if max_samples:
        test_data = test_data[:max_samples]
        print(f"é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ä¸º: {max_samples}")
    
    print(f"æµ‹è¯•æ ·æœ¬æ€»æ•°: {len(test_data)}")
    
    # åˆå§‹åŒ–VLLMåˆ†ç±»å™¨
    classifier = VLLMClassifier(model_name, api_url=api_url)
    
    # å­˜å‚¨ç»“æœ
    results = []
    all_pred_binary = []
    all_true_binary = []
    
    print("å¼€å§‹è¯„ä¼°...")
    for i, item in enumerate(tqdm(test_data, desc="è¯„ä¼°è¿›åº¦")):
        image_path = item['image']
        true_text = item['text']
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            continue
        
        # ç”Ÿæˆé¢„æµ‹
        prediction = classifier.classify(image_path)
        
        # æå–æ ‡ç­¾
        pred_text = extract_answer_content(prediction)
        pred_labels = extract_labels(pred_text)
        true_labels = extract_labels(true_text)
        
        pred_binary = convert_to_binary(pred_labels)
        true_binary = convert_to_binary(true_labels)
        
        all_pred_binary.append(pred_binary)
        all_true_binary.append(true_binary)
        
        # ä¿å­˜ç»“æœ
        result = {
            'index': i,
            'image_path': image_path,
            'true_text': true_text,
            'CoT': prediction,
            'prediction': pred_text,
            'pred_labels': pred_labels,
            'true_labels': true_labels
        }
        results.append(result)
        
        # æ¯å¤„ç†ä¸€ä¸ªæ ·æœ¬å°±è®¡ç®—å¹¶æ‰“å°å½“å‰ç´¯ç§¯æŒ‡æ ‡
        if len(all_pred_binary) > 0:
            y_true_current = np.array(all_true_binary)
            y_pred_current = np.array(all_pred_binary)
            
            current_metrics = calculate_metrics(y_true_current, y_pred_current)
            
            print(f"\n=== æ ·æœ¬ {i+1}/{len(test_data)} ç´¯ç§¯æŒ‡æ ‡ ===")
            print(f"çœŸå®æ ‡ç­¾: {true_labels}")
            print(f"é¢„æµ‹æ ‡ç­¾: {pred_labels}")
            print(f"å½“å‰ç´¯ç§¯æ ·æœ¬æ•°: {len(all_pred_binary)}")
            print(f"å‡†ç¡®ç‡: {current_metrics['accuracy']:.4f}")
            print(f"Hammingå‡†ç¡®ç‡: {current_metrics['hamming_accuracy']:.4f}")
            print(f"F1åˆ†æ•° (Macro): {current_metrics['f1_macro']:.4f}")
            print(f"F1åˆ†æ•° (Micro): {current_metrics['f1_micro']:.4f}")
            print(f"ç²¾ç¡®ç‡ (Macro): {current_metrics['precision_macro']:.4f}")
            print(f"å¬å›ç‡ (Macro): {current_metrics['recall_macro']:.4f}")
            print(f"AUC (Macro): {current_metrics['macro_auc']:.4f}")
            print(f"AUC (Micro): {current_metrics['micro_auc']:.4f}")
            
            # å¯¹äºå‰3ä¸ªæ ·æœ¬ï¼Œæ˜¾ç¤ºæ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
            if i < 3:
                current_per_label_metrics = calculate_per_label_metrics(y_true_current, y_pred_current)
                print_per_label_metrics(current_per_label_metrics, f"å‰{i+1}ä¸ªæ ·æœ¬çš„æ¯ä¸ªæ ‡ç­¾è¯¦ç»†æŒ‡æ ‡")
            
            print("-" * 60)
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    y_true = np.array(all_true_binary)
    y_pred = np.array(all_pred_binary)
    
    final_metrics = calculate_metrics(y_true, y_pred)
    
    # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
    per_label_metrics = calculate_per_label_metrics(y_true, y_pred)
    
    # æ‰“å°æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡
    print_per_label_metrics(per_label_metrics, "æœ€ç»ˆæ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†æŒ‡æ ‡")
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print(f"\nğŸ¯ === æœ€ç»ˆè¯„ä¼°ç»“æœ ===")
    print(f"æ€»æ ·æœ¬æ•°é‡: {len(results)}")
    print(f"å‡†ç¡®ç‡: {final_metrics['accuracy']:.4f}")
    print(f"Hammingå‡†ç¡®ç‡: {final_metrics['hamming_accuracy']:.4f}")
    print(f"F1åˆ†æ•° (Macro): {final_metrics['f1_macro']:.4f}")
    print(f"F1åˆ†æ•° (Micro): {final_metrics['f1_micro']:.4f}")
    print(f"ç²¾ç¡®ç‡ (Macro): {final_metrics['precision_macro']:.4f}")
    print(f"å¬å›ç‡ (Macro): {final_metrics['recall_macro']:.4f}")
    print(f"AUC (Macro): {final_metrics['macro_auc']:.4f}")
    print(f"AUC (Micro): {final_metrics['micro_auc']:.4f}")
    
    # ä¿å­˜ç»“æœ
    if output_file:
        print(f"\nä¿å­˜ç»“æœåˆ°: {output_file}")
        output_data = {
            'model_info': {
                'model_name': model_name,
                'api_url': api_url,
                'total_samples': len(results)
            },
            'final_metrics': final_metrics,
            'per_label_metrics': per_label_metrics,
            'detailed_results': results,
            'summary': {
                'total_samples': len(results),
                'model_name': model_name,
                'test_file': test_file,
                'labels': LABELS
            }
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    return final_metrics, results

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='ä½¿ç”¨VLLMè¯„ä¼°åŒ»ç–—åˆ†ç±»æ¨¡å‹ - 72Bç‰ˆæœ¬')
    parser.add_argument('--model_name', type=str, 
                       default='/data0/zhuoxu/yihong/code/Qwen2.5-VL-72B-Instruct-AWQ', 
                       help='VLLMéƒ¨ç½²çš„æ¨¡å‹åç§°')
    parser.add_argument('--test_file', type=str, 
                       default='/data0/zhuoxu/yihong/code/EasyR1-main/eval_data/test_cls_qwen.json', 
                       help='æµ‹è¯•æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--api_url', type=str, 
                       default='http://127.0.0.1:8000/v1', 
                       help='VLLM APIæœåŠ¡åœ°å€')
    parser.add_argument('--output_file', type=str, 
                       default='/data0/zhuoxu/yihong/code/LLaMA-Factory-main/eval_result/Qwen2.5-VL-72B-VLLM-CLS.json', 
                       help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--max_samples', type=int, help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    # è¿è¡Œè¯„ä¼°
    metrics, results = evaluate_model_vllm(
        model_name=args.model_name,
        test_file=args.test_file,
        api_url=args.api_url,
        output_file=args.output_file,
        max_samples=args.max_samples
    )
    
    print(f"\nğŸ† è¯„ä¼°å®Œæˆï¼ä¸»è¦æŒ‡æ ‡:")
    print(f"Macro AUC: {metrics['macro_auc']:.4f}")
    print(f"F1åˆ†æ•°: {metrics['f1_macro']:.4f}")
    print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    TEST_FILE = "/data0/zhuoxu/yihong/code/EasyR1-main/eval_data/test_cls_qwen.json"
    MODEL_NAME = "/data0/zhuoxu/yihong/code/Qwen2.5-VL-72B-Instruct-AWQ"  # æ ¹æ®å®é™…éƒ¨ç½²çš„æ¨¡å‹åç§°è°ƒæ•´
    API_URL = "http://127.0.0.1:8000/v1"  # VLLMæœåŠ¡åœ°å€
    OUTPUT_FILE = "/data0/zhuoxu/yihong/code/LLaMA-Factory-main/eval_result/Qwen2.5-VL-72B-VLLM-CLS.json"
    
    # è¿è¡Œè¯„ä¼°
    main()